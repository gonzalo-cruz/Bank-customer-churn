---
title: "Bank Customer Churn"
author: "Grupo 6: Gonzalo Cruz Gómez, Samuel Martínez Lorente, Jorge Tordesillas García"
output: 
  html_document:
    theme: flatly
    toc: yes
    toc_float:
      collapsed: true
---

## Indice:

-   Comprensión del problema. Explicación. Lectura de datos. Particiones (Gonzalo)

-   Preparación de datos y EDA (Gonzalo)

-   Técnicas de reducción de la dimensionalidad (Samuel)

    -   PCA con prcomp

    -   PCA manual

-   Aprendizaje no supervisado

    -   Matriz de distancias (Gonzalo y Jorge)

    -   Clustering no jerárquico (Gonzalo y Jorge)

    -   Clustering jerárquico (Gonzalo)

-   Conclusiones (Gonzalo, Samuel, Jorge)

## Comprensión del problema. Explicación. Lectura de datos. Particiones

Objetivo:

El objetivo principal de este conjunto de datos es predecir la fuga de clientes (churn). En otras palabras, queremos construir un modelo que pueda determinar qué clientes tienen más probabilidades de dejar el banco en el futuro. Esto es importante para los bancos, ya que retener a los clientes existentes es normalemente más rentable que conseguir clientes nuevos.

Descripción de Datos (Variables):

```         
CustomerId: Identificador único para cada cliente. (Tipo: Numérico)
Credit_score: Puntaje de crédito del cliente. (Tipo: Numérico)
Country: País donde reside el cliente (por ejemplo, Francia, España, Alemania). (Tipo: Categórico)
Gender: Género del cliente (Masculino o Femenino). (Tipo: Categórico)
Age: Edad del cliente. (Tipo: Numérico)
Tenure: Número de años que el cliente ha sido cliente del banco. (Tipo: Numérico)
Balance: Saldo en la cuenta bancaria del cliente. (Tipo: Numérico)
Products_number: Número de productos bancarios que el cliente utiliza. (Tipo: Numérico)
Credit_card: Indica si el cliente tiene tarjeta de crédito (1 = Sí, 0 = No). (Tipo: Binario)
Active_member: Indica si el cliente es un miembro activo (1 = Sí, 0 = No). (Tipo: Binario)
EstimatedSalary: Salario estimado del cliente. (Tipo: Numérico)
Churn: Variable objetivo. Indica si el cliente dejó el banco (1 = Sí, 0 = No). (Tipo: Binario)
```

Tipo de Problema:

Este es un problema de clasificación binaria. El objetivo es clasificar a los clientes en dos categorías: aquellos que se irán (1) y aquellos que se quedarán (0).

### Business understanding

Planteamos preguntas sobre nuestros datos:

-   ¿Influye el salario en si el cliente deja el banco?

-   ¿Influye el tiempo que lleva el cliente en el banco en si este deja este banco?

-   ¿Influye la edad?

-   ¿Influye el género?

-   ¿Influye el país?

### Data understanding y división de los datos

Importamos las librerías

```{r librerias}
library(ggplot2)
library(readr)
library(tidyr)
library(dplyr)
library(gridExtra)
library(GGally)
library(factoextra)
library(cluster)
library("NbClust")
library(parameters)
library(tidyverse)
library(corrplot)
```

Leemos los datos y vemos sus dimensiones

```{r lectura de datos}
Data <- read_csv("Bank Customer Churn Prediction.csv")
ntotal <- dim(Data)[1]
ptotal <- dim(Data)[2]
```

```{r visualizar primeros datos de la tabla}
head(Data)
```

Podemos ver que tenemos n = 10000 observaciones y 12 variables en el dataset

Dividimos los datos en train-test-validate

```{r division train-test-val}
set.seed(123)

# creamos los indices
indices <- 1:ntotal
ntrain <- ntotal *.6
ntest <- ntotal* .2
nval <- ntotal-(ntrain+ntest)
indices.train <- sample(indices, ntrain, replace= FALSE)
indices.test <- sample(indices[-indices.train], ntest, replace= FALSE)
indices.val <- indices[-c(indices.train, indices.test)]

# 60% para train, 20% para test y 20% para validate

train <- Data[indices.train,]
test <- Data[indices.test,]
validate <- Data[indices.val,]
```

Veamos las variables:

```{r visualizar variables}
str(train)
```

Podemos observar que la mayoria de nuestras variables son numéricas, a excepción de aquellas que son char como country y gender, que son variables categóricas. En el caso de credit card, active_member, products number, tenure y churn se trata de variables discretas.

Antes de nada vamos a transformar nuestras variables categóricas de char a factor.

```{r}
train$country <- as.factor(train$country)
train$gender <- as.factor(train$gender)
```

## Exploratory Data Analysis

Veamos un resumen de algunos de los estadísticos principales de cada columna

```{r estadísticos resumen}
summary(train)
```

Comprobamos si hay valores faltantes:

```{r valores faltantes}
colSums(is.na(Data))
```

Vemos que no tenemos valores faltantes en ninguna columna.

Visualizamos nuestros datos para ver cómo son y cómo están distribuidos

```{r visualizacion variables continuas}

train_long <- train %>%
  dplyr::select(estimated_salary, balance, credit_score) %>%  
  tidyr::gather(key = "Variable", value = "Value")

ggplot(train_long, aes(x = Value, fill = Variable)) +
  geom_histogram(bins = 20, color = "black", alpha = 0.5, aes(y =..density..)) + 
  geom_density(aes(y =..density.., color = Variable), linewidth = 0.5, alpha = 0.2) + 
  facet_wrap(~ Variable, scales = "free") +
  labs(title = "Continuous variables", x = "Value", y = "Density") +
  scale_fill_manual(values = c("skyblue", "lightgreen", "lightcoral")) +
  scale_color_manual(values = c("darkblue", "darkgreen", "darkred")) + 
  theme_bw() +
  theme(legend.position = "none")
```

Nuestras variables balance y credit score siguen distribuciones bastante similares a la normal (exceptuando el gran número de ceros en balance), mientras que el salario estimado sigue una distribución más uniforme.

```{r visualizacion variables discretas}

  train_long <- train %>%
    tidyr::pivot_longer(cols = c(products_number, active_member, churn),
                        names_to = "variable", values_to = "value")

  combined_plot <- ggplot(train_long, aes(x = value)) +
    geom_bar(fill = "lightblue", color = "black") +
    facet_wrap(~ variable, scales = "free_x") +
    labs(title = "Discrete variables",
         x = "Value",
         y = "Count") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))  
  print(combined_plot)
  
```

Podemos ver como hay más gente que se queda en el banco respecto a la que se va, el numero de miembros activos y pasados es basicamente igual y que el número de productos más habitual es o 0 o 1, con muy pocos clientes teniendo más de 2.

```{r visualizacion tenure}
plot_tenure_hist <- ggplot(train, aes(x = tenure)) +
  geom_histogram(binwidth = 1, fill = "lightblue", color = "black") +  
  labs(title = "Distribution of Tenure",
       x = "Tenure",
       y = "Count") +
  theme_bw()

print(plot_tenure_hist)
```

Tenemos una distribución bastante uniforme en el tiempo que está cada cliente en el banco.

Ahora miramos las variables categóricas

```{r}
# Gráfico de barras para 'Gender'
ggplot(train, aes(x = gender)) +
  geom_bar(fill = c("skyblue", "salmon")) +
  labs(title = "Distribución de Género",
       x = "Género",
       y = "Número de Clientes") +
  theme_minimal()

# Gráfico de barras para 'Geography'
ggplot(train, aes(x = country)) +
  geom_bar(fill = c("lightgreen", "lightcoral", "lightblue")) +
  labs(title = "Distribución por Países",
       x = "País",
       y = "Número de Clientes") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotar etiquetas

```

Podemos ver que hay más clientes hombres que mujeres, aunque la diferencia no es grande, y también podemos observar que la mayor parte de los clientes del banco son franceses, con un número similar de clientes de este país a la combinación de España y Alemania.

Vamos a buscar si hay algún tipo de relación entre nuestras variables, para ello vamos a utilizar scatter plots:

```{r}
# Estimated salary vs Balance
ggplot(train, aes(x = estimated_salary, y = balance)) +
  geom_point() +
  labs(x = "Estimated Salary", y = "Balance")

# Balance vs Credit Score
ggplot(train, aes(x = credit_score, y = balance)) +
  geom_point() +
  labs(x = "Credit Score", y = "Balance")

# Credit Score vs Estimated Salary
ggplot(train, aes(x = estimated_salary, y = credit_score)) +
  geom_point() +
  labs(x = "Estimated Salary", y = "Credit Score")


```

Podemos ver como siguen distribuciones bastante normales, con la excepción de los datos con valor 0 del balance, que como hay muchos, tienen mucho peso.

Hacemos una matriz de correlación para ver como están relacionadas nuestras variables entre ellas:

```{r}
train_filtered <- train %>% select(-country, -gender)
correlation_matrix <- cor(train_filtered)
corrplot(correlation_matrix, method = "color", type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45) 
```

Podemos ver como el balance y el número de productos están bastante relacionados, de manera inversa, cuanto menos dinero, más productos, además la edad está bastante relacionada con que un cliente abandone el banco.El resto de variables no tienen demasiada relación entre ellas.

Vamos a ver qué nos dice nuestra variable objetivo

```{r visualización churn}
ggplot(data = train, aes(x = churn, fill = factor(churn))) + 
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  scale_y_continuous(labels = scales::percent) +
  theme(legend.position = "none") +
  ylab("Relative Frequency") +
  xlab("Churn") +
  theme_minimal() +  
  scale_fill_manual(values = c("skyblue", "tomato"), 
                    labels = c("No", "Yes")) + 
  labs(title = "Churn Distribution")
```

Podemos ver como la mayoría de los clientes no se van del banco pero hay un porcentaje significativo que si lo hace

Ahora veamos como estan distribuidos por género y país:

```{r churn vs genero}
train$churn <- factor(train$churn, levels = c(0, 1), labels = c("No", "Yes"))

train %>%
  count(gender, churn) %>%
  mutate(Percentage = n / sum(n) * 100) %>%
  ggplot(aes(x = gender, y = Percentage, fill = churn)) +
  geom_col(position = "dodge") +
  labs(title = "Churn by Gender", x = "Gender", y = "Percentage") +
  theme_bw() +
  scale_fill_manual(values = c("skyblue", "tomato"), name = "Churn Status")

```

Aquí se observa que el porcentaje de hombres que no se marchan del banco es mayor que el de las mujeres

```{r}
train %>%
  count(country, churn) %>%
  mutate(Percentage = n / sum(n) * 100) %>%
  ggplot(aes(x = country, y = Percentage, fill = churn)) +
  geom_col(position = "dodge") +
  labs(title = "Churn by country", x = "country", y = "Percentage") +
  theme_bw() +
  scale_fill_manual(values = c("skyblue", "tomato"), name = "Churn Status")
```

Vemos como los clientes franceses y los españoles son menos propensos que los alemanes a marcharse del banco

Vamos a intentar contestar las preguntas que nos hicimos al inicio:

- ¿Influye el salario en si el cliente deja el banco?

```{r churn vs salario}
ggplot(train, aes(x = factor(churn), y = estimated_salary, fill = factor(churn))) +
  geom_boxplot() +
  labs(title = "Salary vs. Churn", x = "Churn", y = "Estimated Salary") +
  scale_x_discrete(labels = c("No", "Yes")) +
  scale_fill_manual(values = c("skyblue", "tomato"), labels = c("No", "Yes")) +
  theme_bw() +
  theme(legend.position = "none", plot.title = element_text(hjust = 0.5)) 

```

Podemos ver como, a priori, el salario anual no influye en la decisión de abandonar el banco. Vamos a comprobarlo con un contraste de hipótesis.

Primero vemos si sigue una distribución normal para saber qué test aplicar después.. Para ello utilizamos el test de shapiro:

```{r distribucion salary}
train$churn <- as.factor(train$churn)
alpha <- 0.05
train %>%
  group_by(churn) %>%
  summarize(p_value = shapiro.test(estimated_salary)$p.value)

```

Como no sigue una distribución normal debemos utilizar un test no paramétrico coomo el de wilcoxon para comprobar si existe una diferencia significativa.

```{r contraste churn-salario}

wilcox_result <- wilcox.test(estimated_salary ~ churn, data = train)
print(wilcox_result)
alpha <-  0.05
p_value <- wilcox_result$p.value

```

Como ya podíamos intuir por la gráfica no hay una diferencia significativa respecto al salario

\- ¿Influye el tiempo que lleva el cliente en el banco en si este deja el banco?

```{r churn vs tenure}
ggplot(train, aes(x = factor(churn), y = tenure, fill = factor(churn))) + 
  geom_boxplot() +
  labs(title = "Tenure vs. Churn", x = "Churn", y = "Tenure (years)") +
  scale_x_discrete(labels = c("No", "Yes")) + 
  scale_fill_manual(values = c("skyblue", "tomato"), labels = c("No", "Yes")) +
  theme_bw() +
  theme(legend.position = "none", plot.title = element_text(hjust = 0.5))  
```

Vamos a ver si hay una diferencia significativa entre los dos grupos. Para ello vamos a plantear un contraste de hipótesis:

Primero comprobamos si tenure sigue una distribución normal

```{r distribucion tenure}
train$churn <- as.factor(train$churn)
alpha <- 0.05
train %>%
  group_by(churn) %>%
  summarize(p_value = shapiro.test(tenure)$p.value)

```

Sigue una distribución normal. Utilizamos un test t para comprobar si existe alguna diferencia significativa.

```{r contraste tenure-churn}
t_test_result <- t.test(tenure ~ churn, data = train)
print(t_test_result)

p_value <- t_test_result$p.value


```

Por tanto podemos decir que el hecho de abandonar o no el banco sí depende del tiempo que llevan los clientes en el mismo.

\- ¿Influye la edad?

```{r age vs churn}
ggplot(train, aes(x = factor(churn), y = age, fill = factor(churn))) + 
  geom_boxplot() +
  labs(title = "Age vs. Churn", x = "Churn", y = "Age (years)") +
  scale_x_discrete(labels = c("No", "Yes")) + 
  scale_fill_manual(values = c("skyblue", "tomato"), labels = c("No", "Yes")) +
  theme_bw() +
  theme(legend.position = "none", plot.title = element_text(hjust = 0.5))
```

Vamos a comprobar si es normal para ver qué test utilizamos:

```{r distribucion age}
train %>%
  group_by(churn) %>%
  summarize(p_value = shapiro.test(age)$p.value)
```

Vemos que age no sigue una distribución normal, por lo que aplico un test de wilcoxon

```{r }
wilcox_result <- wilcox.test(age ~ churn, data = train)
print(wilcox_result)
p_value <- wilcox_result$p.value
if (p_value < alpha) {
  print("Rechazamos la hipótesis nula. Hay una diferencia significativa en el tiempo que son clientes entre aquellos que abandonan el banco y aquellos que no")
} else {
  print("No podemos rechazar la hipótesis nula. No existe esta diferencia significativa")
}
```
```{r contraste age-churn}
wilcox_result <- wilcox.test(age ~ churn, data = train)
print(wilcox_result)
p_value <- wilcox_result$p.value
if (p_value < alpha) {
  print("Rechazamos la hipótesis nula. Hay una diferencia significativa con la edad de los clientes y los que abandonan el banco y aquellos que no")
} else {
  print("No podemos rechazar la hipótesis nula. No existe esta diferencia significativa")
}
```

Podemos ver como la edad importa a la hora de que un cliente abandone el banco o no, usualmente siendo los clientes mayores los que tienden a abandonar el banco

-   ¿Influye el balance?

```{r balance vs churn}
ggplot(train, aes(x = factor(churn), y = balance, fill = factor(churn))) + 
  geom_boxplot() +
  labs(title = "Balance vs. Churn", x = "Churn", y = "Balance") +
  scale_x_discrete(labels = c("No", "Yes")) + 
  scale_fill_manual(values = c("skyblue", "tomato"), labels = c("No", "Yes")) +
  theme_bw() +
  theme(legend.position = "none", plot.title = element_text(hjust = 0.5))
```

Vamos a comprobar si es normal para ver qué test utilizamos:

```{r distribucion balance}
train %>%
  group_by(churn) %>%
  summarize(p_value = shapiro.test(age)$p.value)
if(p_value<alpha){
  print("No sigue una distribución normal")
}else{
  print("sigue una distribución normal")
}
```

```{r contraste balance-churn}
wilcox_result <- wilcox.test(balance ~ churn, data = train)
print(wilcox_result)
p_value <- wilcox_result$p.value
if (p_value < alpha) {
  print("Rechazamos la hipótesis nula. Hay una diferencia significativa en el balance de los clientes entre aquellos que abandonan el banco y aquellos que no")
} else {
  print("No podemos rechazar la hipótesis nula. No existe esta diferencia significativa")
}
```

Se observa que cuanto mayor es el balance, mayor es la probabilidad de que el cliente se cambie de banco

## Técnicas de reducción a la dimensión (PCA).

### PCA con prcomp

Con el siguiente código veremos cómo afecta el uso del PCA a nuestros datos. Para utilizar PCA únicamente utilizamos los datos continuos de nuestro dataset

```{r}
set.seed(1234)

data_cont <- train[, c("credit_score", "age", "tenure", "balance", "products_number", "estimated_salary")]
data_cont <- na.omit(data_cont)

pca <- prcomp(data_cont, center = TRUE, scale. = TRUE)

# Guardar nuevas varialbes
data_pca <- as.data.frame(pca$x)
colnames(data_pca) <- paste0("PC", 1:ncol(data_pca))  # Renombrar dinámicamente

# Gráfico de datos originales (matriz de dispersión)
p1 <- ggpairs(data_cont) + 
  ggtitle("Datos originales (variables continuas)")

# Gráfico de componentes principales
p2 <- ggplot(data_pca, aes(x = PC1, y = PC2)) +
  geom_point(color = "red", alpha = 0.6) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  coord_fixed() +
  ggtitle("Datos transformados (PCA)")

# Mostrar gráficos lado a lado
# Mostrar gráficos de forma separada
print(p1)  # Matriz de dispersión creada con ggpairs
print(p2)  # Gráfico de componentes principales
```

Ahora los resultados de aplicar el PCA usando el summary para ver el resumen:

```{r}
# Resumen del PCA 
pca_summary <- summary(pca)
print(pca$sdev)
print(pca$rotation)
print(pca$center)
print(pca$scale)
print(pca_summary)
```

Podemos ver que nuestras componentes principales muestran lo siguiente:

-   PC1 es la componente principal más importante, nos muestra un 21% de la información del dataset, explica la mayor parte de la varianza y está relacionada principalmente con el credit score y balance de forma negativa y con el número de productos de forma positiva. Esto quiere decir que los clientes con un mayor número de productos van a ir por un lado, mientras que los que tengan mayor credit score y balance ìrán por el otro.

-   PC2 es la siguiente más importante, nos muestra un 17% de la información y está asociada con la edad de forma negativa y con el salario estimado de forma positiva, por lo que muestra una relación entre la edad y el salario estimado. Esto quiere decir que los clientes mayores irían por un lado, mientras que los que tienen salarios altos irían por el otro.

```{r}
prcomp(Data$credit_score)
```

### PCA manual

Sin usar ninguna función de R para reducir la dimensionalidad lo haremos de la siguiente manera: 

1er paso: Realizaremos la normal de nuestro conjunto de datos, los escalamos y los centramos:

```{r}
set.seed(1234)

# tomamos los datos continuos
data_cont <- train[, c("credit_score", "age", "tenure", "balance", "products_number", "estimated_salary")]
data_cont <- na.omit(data_cont)

X_data <- as.matrix(data_cont)

# Calculamos las medias de cada columna.
medias_data <- colMeans(X_data)

# Calculamos la desviación típica de cada columna.
desv_data <- apply(X_data,2,sd)

# Una vez tenemos los datos podemos normalizar nuestra variable.
# Primero restamos la media y después dividimos por la norma.
X_data <- sweep(X_data, 2, medias_data, "-")
X_norm <- sweep(X_data, 2, desv_data, "/")
```

2º paso: Calculamos la matriz de covarianza.

```{r}
C <- cov(X_norm)
print(C)
```

3er paso: Calculamos los autovalores y autovectores, usamos "eigen"

```{r}
# Descomponemos
eig <- eigen(C, TRUE, only.values = FALSE, EISPACK = FALSE)

# Accedemos a los autovalores y autovectores
autovalores <- eig$values
autovectores <- eig$vectors
print(autovectores)
print(autovalores)
```

Podemos ver como el resultado es el mismo que aplicando la función prcomp

## Aprendizaje no supervisado

Vamos a ver cómo se agrupan los clientes en base a una serie de parámetros, para ello vamos a utilizar clustering, tanto jerárquico como no jerárquico.

### Matriz de distancias

En primer lugar vamos a calcular la distancia Euclídea entre las observaciones de la base de datos. También mostraremos la matriz de distancias.

```{r}
# Hacemos un escalado de las variables continuas
datos_escalados <- scale(data_cont)
```

```{r}
head(datos_escalados)
```

Matriz de distancias

```{r}
distance <- get_dist(datos_escalados, method = "euclidean")
print(as.matrix(distance)[1:5, 1:5])

# para visualizarlo, vamos a utilizar una muestra para optimizar el rendimiento
tamaño_muestra <- floor(0.1 * nrow(datos_escalados))
sampled_data <- datos_escalados[sample(seq_len(nrow(datos_escalados)), size = tamaño_muestra), ]

distance_matrix_sampled <- get_dist(sampled_data, method = "euclidean")

fviz_dist(distance_matrix_sampled) 

```

A partir de esto, podemos observar como se cumplen las condiciones para ser una métrica o medida de desemejanza:

-   Coincidencia: podemos ver que cuando x=y el valor es 0

-   No negatividad: el valor más bajo de nuestra matriz de distancias es 0

-   Simetría: con el gráfico podemos ver como la matriz es simétrica respecto al eje formado por x = y

### Clustering no jerárquico

Para empezar, podemos aplicar el algoritmo de las k-medias con k = 2, luego iremos ajustando el número de clusters para llegar al óptimo

```{r}
k_medias <- kmeans(datos_escalados, centers = 2, nstart = 25)
str(k_medias)
```

Si imprimimos los resultados vemos que la técnica de agrupaciones dio lugar a 2 conglomerados o medias para los dos grupos en las variables que haya. También obtenemos la asignación de conglomerados para cada observación.

```{r}
set.seed(595)
k_medias
```

Visualización de los clusters

```{r}
fviz_cluster(k_medias, data = datos_escalados, geom = "point")
```

Vamos a realizar los 3 métodos más populares para determinar el número óptimo de clústeres:

MÉTODO DEL CODO

```{r}
set.seed(123)

fviz_nbclust(datos_escalados, kmeans, method = "wss")
```

En este análisis, a partir de 4 clusters, la reducción en la suma total de cuadrados internos parece estabilizarse, indicando que k = 4 es una buena opción.

```{r}
set.seed(595)
k4 <- kmeans(datos_escalados, centers = 4, nstart = 25)

fviz_cluster(k4, data = datos_escalados, geom = "point")
k4

```

Del plot de los clusters podemos ver como este numero de clusters no es ideal, ya que se mezclan demasiado entre ellos, sin haber una diferenciación clara entre clusters.

```{r}
set.seed(595)
table(k_medias$cluster, k4$cluster)

```

MÉTODO DE LA SILUETA

```{r}

fviz_nbclust(datos_escalados, kmeans, method = "silhouette")

```

Con esta técnica, en nuestros datos, parece que la mejor elección es k igual a 2.

```{r}
sil <-  silhouette(k_medias$cluster, dist(datos_escalados))

fviz_silhouette(sil)
```

La interpretación del coeficiente de la silueta la entendemos como que un valor positivo significa que la observación está bien agrupada. Cuanto más se acerque al coeficiente a 1, mejor agrupada estará la observacíon. En cambio, un valor negativo significa que la obervación está mal agrupada. Finalmente, un valor igual a 0 significa que la observación se encuentra entre dos conglomerados.

El gráfico anterior y el coeficiente de silueta medio ayudan a determinar si la agrupación es buena o no.

MÉTODO GAP

El proceso es el siguiente: se aplica el algoritmo de clustering a los datos con diferentes valores de k, se generan conjuntos de datos de referencia aleatorios, se calcula el estadístico Gap para cada valor de k y la selección del número óptimo de clusters.

```{r}
set.seed(123)
# como el dataset es muy grande, vamos a utilizar una muestra de nuestro dataset
gap <-  clusGap(sampled_data, FUN = kmeans, K.max = 10, nstart = 25, B = 50)

print(gap, method = "firstmax")
```

```{r}
fviz_gap_stat(gap)
```

Parece ser que el número óptimo de clusters según este método es 1, algo que no tiene demasiado sentido, ya que nos interesa clasificar los datos en distintos grupos, no solo en 1.

```{r}
k1 <-  kmeans(datos_escalados, centers = 1, iter.max = 10, nstart = 25)

fviz_cluster(k1, data = datos_escalados, geom = "point")
```

Esquema de agrupación a partir de los diferentes resultados obtenidos variando todas las combinaciones de clústeres, medias de distancia y métodos de agrupación

```{r}
# de nuevo para facilitar el procesamiento utilizamos una muestra del dataset
 nb <-  NbClust(sampled_data, distance = "euclidean", min.nc = 2, max.nc = 10, method = "kmeans")
```

Las dos primeras gráficas me indican que, a más clusters, mejor es la estadística de Hubert y mejor es el ajuste de los clusters, además nos dice que el mejor número de clusters respecto a dicha estadística es 3 y que a partir de 4 ya deja de tener sentido hacer más clusters. A pesar de esto, como hemos podido ver con los 4 clusters, perdemos mucha explicabilidad, ya que se empiezan a mezclar los clusters, por lo que, aunque respecto a la estadística de Hubert es lo más apropiado, puede no ser el número óptimo de clusters. Las siguientes dos gráficas nos dicen lo siguiente:

-   El primero nos dice que según la métrica de la silueta, a más clusters, peor, como hemos podido observar anteriormente.

-   La última nos dice que el mejor número de clusters en este sentido es 2 y que a partir de ahí, el clustering empieza a ser peor.

```{r}
n_clust <-  n_clusters(as.data.frame(sampled_data), 
                       package = c("easystarts", "NbClust", "mclust"),
                       standardize = FALSE)
n_clust
```

```{r}
plot(n_clust)
```

Esta función, que compara el número de clusters óptimo obtenido mediante varios métodos, nos dice claramente que el mejor número es 3.

```{r}
set.seed(100)
k3 <-  kmeans(datos_escalados, centers = 3, iter.max = 10, nstart = 25)

fviz_cluster(k3, data = datos_escalados, geom = "point")
```

Podemos ver como hay tres conjuntos relativamente bien diferenciados. Hay solapamiento ya que nuestras dos primeras componentes principales no muestran demasiada información del dataset. Tendríamos que representarlo en más dimensiones para poder verlo mejor

```{r}
data_cont %>%
  mutate(clusters = k3$cluster) %>%
  group_by(clusters) %>%
  summarise_all("mean")
```

Podemos interpretar esta tabla de diferenciación de las medias de la siguiente manera:

-   El primer cluster es de la gente que lleva menos tiempo en el banco, pero que tiene mucho dinero metido en el y tienen el mejor credit score

-   El segundo es gente que lleva más tiempo que los anteriores en el banco, ganan más o menos lo mismo que los anteriores, pero su balance es mucho menor. También son los que más productos tienen del banco. Su credit score es intermedio

-   Por último, el tercer cluster se compone de la gente que más tiempo lleva en el banco, más dinero tiene y más gana, pero con el menor credit score.

```{r}
res_kmeans <-  cluster_analysis(datos_escalados, 
                                n = 3,
                                method = "kmeans")

plot(summary(res_kmeans))
```

Este análisis del cluster confirma nuestras conclusiones extraídas de la tabla de diferenciación.

A continuación vamos a probar con k-medioides en vez de con k-medias.

Primeramente utilizamos el metodo pam, con distancia manhattan en vez de la distancia euclídea

```{r}
set.seed(100)
pam_cluster <- pam(x = datos_escalados, k = 3, metric = "manhattan")
pam_cluster

fviz_cluster(object = pam_cluster, data = datos_escalados, ellipse = TRUE, 
             repel = TRUE) +
  theme_bw() +
  labs(title = "Resultados clustering PAM") + theme(legend.position = "none")
```

Podemos ver que no es un método idóneo, ya que hay mucho solapamiento entre clusters. Vamos a probar con el método de los clara clusters

```{r}
set.seed(100)
clara_clusters <- clara(x = datos_escalados, k = 3, metric = "manhattan", stand = TRUE,
                        samples = 50, pamLike = TRUE)
clara_clusters
```

Visualizamos:

```{r}
set.seed(100)
fviz_cluster(object = clara_clusters, ellipse.type = "t", geom = "point",
             pointsize = 2.5) +
  theme_bw() +
  labs(title = "Resultados clustering CLARA") +
  theme(legend.position = "none")
```

Podemos ver como es bastante más parecido al resultado obtenido con k-means, aunque aquí se puede ver como solamente toma una muestra del dataset completo.

### Clustering jerárquico

Después de haber hecho clustering no jerárquico con k-medias y k-medioides, obteniendo resultados similares, vamos a aplicar métodos de clustering jerárquico.

Vamos a empezar por el clustering jerárquico algomerativo, para ello vamos a evaluar todos los métodos para ver cual es el que mejor coeficiente de agrupamiento nos da.

```{r}
set.seed(100)
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# Calculamos el coeficiente de agrupamiento de todos los métodos
ac <- function(x) {
  agnes(sampled_data, method = x)$ac
}

map_dbl(m, ac)
```

Podemos ver como el método Ward nos dá el mayor coeficiente de agrupamiento. Dado el gran número de observaciones que tenemos, elaborar un dendograma no tiene ningún sentido, ya que no solo sería ilegible, sino que también su coste computacional sería muy alto.

```{r}
hc1 <- hclust(distance, method = "ward.D2")
hc1
```

Hacemos como con el clústering no jerárquico, primeramente visualizamos 2 clusters y luego veremos el número idóneo de clusters con diferentes métodos.

```{r}
set.seed(100)
clusters <- cutree(hc1, k = 2)
pca_df <- as.data.frame(pca$x[, 1:2]) 

pca_df$cluster <- as.factor(clusters)

# Hacemos un scatter plot para visualizar los clusters. 
ggplot(pca_df, aes(x = PC1, y = PC2, color = cluster)) + 
  geom_point() +
  labs(title = "Scatter Plot of Clusters", x = "PC1", y = "PC2")

```

Vamos a hacer el clustering jerárquico divisivo para ver el número de clusters

```{r}
hc2 <- diana(sampled_data)
hc2$dc
```

Vamos a visualizar un dendrograma, aunque no nos será muy útil:

```{r}
pltree(hc2, cex = 0.6, hang = -1, main = "Dendrogram de DIANA")
```

Como ya habíamos dicho antes, hacer un dendograma no tiene mucho sentido ya que, al tener tantas observaciones, no se ve demasiado, lo que si podemos ver es que si cortamos a una altura de 6 nos quedan 4 clusters medianamente bien diferenciados. Vamos a aplicar mejor los métodos que aplicamos en clustering no jerárquico para ver el número óptimo de clusters.

Primeramente el método del codo:

```{r}
fviz_nbclust(datos_escalados, FUN = hcut, method = "wss")
```

Podemos ver como aquí el número óptimo de clusters parece ser 4

Vamos a aplicar el método gap, para ello vamos a utilizar una muestra del dataset:

```{r}
set.seed(100)
gap_stat <- clusGap(sampled_data, FUN = hcut, nstart = 25, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
```

Como nos pasaba en el clustering no jerárquico, también nos dice que el número óptimo es 1 cluster, algo que no nos sirve, vamos a ver otros métodos como el NbClust:

```{r}
 nb2 <-  NbClust(sampled_data, distance = "euclidean", min.nc = 2, max.nc = 10, method = "ward.D2")
```

Según este método, nuestro número ideal de clusters es 2 o 3, como podemos ver en las gráficas.

A continuación vamos a utilizar n_clusters con NbClust únicamente, ya que es el paquete que más se centra en clustering jerárquico

```{r}
n_clust2 <-  n_clusters(as.data.frame(sampled_data), 
                       package = c("NbClust"),
                       standardize = FALSE)
n_clust2
plot(n_clust2)
```

Podemos ver como nos indica que el número idóneo de clusters es 3.

Por ello, vamos a cortar en 3 clusters nuestros datos

```{r}
set.seed(100)
hc3 <- hclust(distance, method = "ward.D2" )

sub_grp <- cutree(hc3, k = 3)
table(sub_grp)
```

De aqui podemos extraer que el segundo cluster contiene bastantes más puntos que los otros dos.

Vamos a visualizar nuestros clusters:

```{r}
set.seed(100)
fviz_cluster(list(data=datos_escalados,cluster=sub_grp), geom = "point")
```

A pesar de ser 3 clusters igual que en el clustering no jerárquico, el resultado es diferente al que obtuvimos en este, vamos a ver qué nos dicen estos clusters.

```{r}
cluster_means <- datos_escalados %>%
  as.data.frame() %>%  
  mutate(clusters = sub_grp) %>%
  group_by(clusters) %>%
  summarise_all(mean)

print(cluster_means)

```

Podemos ver como ahora los clusters se dividen de la siguiente manera:

-   El primer cluster está compuesto por la gente que tiene un credit score ligeramente más alto, son un poco más jovenes que la media, tienen muchos productos y bastante dinero en el banco.

-   El segundo contiene a gente algo más mayor, con un credit score promedio, un balance alto en el banco, pero utilizan menos productos.

-   Por último, el tercero está compuesto por aquellos que son ligeramente más jóvenes que la media, tienen un credit score más bajo, un balance considerablemente más bajo y utilizan menos productos del banco.

## Conclusiones

Primeramente hemos podido ver que nuestro dataset está muy completo, sin datos faltantes ni outliers que nos puedan dificultar nuestro análisis de los mismos.
Al observar las distribuciones de nuestro dataset, hemos visto que nuestras variables contínuas siguen en su mayoría distriones cercanas a la normal, además de tener bastantes variables binarias que nos pueden dar mucha información a futuro cuando apliquemos algoritmos de clasificación binaria para preedecir si un cliente va o no a abandonar el banco.

Podemos observar que la tasa de personas que se mantienen en el banco es mayor que la de la gente que se va, también vemos las relaciones de cada variable con el abandono del banco y podemos ver como algunas de las variables más relacionadas con el abandono son la edad o el balance, que muestra el dinero restante de cada cliente en el banco.

Podemos ver  hay cerca de un 80% de los clientes del banco que no lo abandonan y poco más del 20% que sí, de esa tasa de abandono cercana al 20% son más mujeres que hombres las que abandonan el banco, en cambio si filtramos por países los alemanes son más propensos a irse del banco.

La relación del dinero con el banco es interesante, no solo el dinero que tienen en el banco si no el salario que estimamos que tiene cada cliente con su permanencia en el banco, podemos ver que realmente el salario estimado no tiene relacion alguna ni con el abandono ni con la permanencia de los cliente, mientras que el balance sí que importa para que una persona se quede en el banco o no, ya que a mayor balance, podemos observar una mayor tendencia a abandonar el banco.
Respecto a la edad y el tiempo que llevan en el banco vemos que también influyen, a mayor edad la tasa de abandono es mayor.

En cuanto al PCA, nuestras dos primeras componentes principales nos muestran solamente un 40% aproximado de la información, por lo todos nuestros clusters, independientemente del método que utilicemos, se van a ver solapados, ya que no tienen mucha cantidad de información del dataset. Si visualizáramos en más dimensiones quedaría más clara la separación de los clusters. 

También en relación con el PCA, estamos utilizando solamente las variables continuas, por lo que como  nuestro dataset tiene bastantes variables binarias, asi como dos variables categóricas, estamos perdiendo información que podría ser valiosa a la hora de clasificar nuestros datos en diferentes clusters. Para ello deberíamos utlizar otros métodos como el escalado multidimensional o algún otro tipo de selección de variables, algo que sería interesante implementar de cara al aprendizaje supervisado, para poder predecir con toda la información de nuestro dataset si una persona se irá o no del banco.

También para reducir la dimensionalidad podríamos utilizar los mapas auto-organizados, que utilizando clustering nos podrían ayudar a hacer una selección de variables aún más precisa.

Asimismo, nuestra matriz de distancias está computada con las distancias euclídeas entre nuestros datos contínuos, ganaríamos más precisión si creáramos nosotros una matriz de distancias teniendo en cuenta estas variables binarias también, además de las variables categóricas. 

En cuanto a los distintos métodos de clustering, tanto para clustering jerárquico como no jerárquico, vemos que el número de clusters es igual, a pesar de eso, los clusters que obtenemos mediante los métodos jerárquicos y no jerárquicos. Con k-means y k-medioides con el método clara tenemos un resultado bastante similar y con los métodos jerárquicos y el método PAM tenemos resultados también similares, pero diferentes a los anteriores. Estos resultados se diferencian entre si por el criterio que utilizan a la hora de separar los valores. Mientras que el clustering jerárquico separa los puntos por edad, credit score, balance y productos, siendo bastante importantes a la hora de diferenciar los conjuntos el número de productos, el balance y el credit score; el clustering no jerárquico da mayor importancia a al tiempo que llevan los clientes en el banco (tenure), el balance de las cuentas y el credit score. Esto se asemeja a las primeras conclusiones que pudimos extraer del dataset al hacer el EDA, donde pudimos ver que el balance, la edad y el tiempo que llevan los clientes en el banco son factores importantes y diferenciales. 

Estos grupos pueden servir después para ver qué grupo es más probable que abandone el banco en algún momento en base al grupo al que pertenezca, esto lo podremos hacer con los métodos de aprendizaje supervisado que veremos más adelante en la asignatura, con los que podremos predecir esto


